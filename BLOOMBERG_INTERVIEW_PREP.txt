================================================================================
BLOOMBERG INTERVIEW PREPARATION GUIDE
================================================================================

PROJECT OVERVIEW
================================================================================

Project Name: Volunteer Management System
Tech Stack: React (Frontend) + Express.js (Backend) + MySQL (Database)

ELEVATOR PITCH (30 seconds):
"I built a full-stack volunteer management system that connects volunteers 
with events based on skill matching. The system includes user authentication 
with JWT, a notification system for real-time updates, and an intelligent 
matching algorithm that considers volunteer skills against event requirements. 
The backend is a REST API built with Express and MySQL, while the frontend 
uses React with React Router for navigation."


================================================================================
TECHNICAL DEEP DIVES
================================================================================

1. NOTIFICATION SYSTEM - TECHNICAL CHALLENGES
================================================================================

ARCHITECTURE:
- Database-driven notification system using a 'notifications' table with 
  foreign keys to 'users' and 'events'
- RESTful endpoints for fetching and creating notifications
- User-specific filtering via query parameters

KEY TECHNICAL CHALLENGES FACED:

Challenge 1: Many-to-Many Relationship Complexity
---------------------------------------------------
Problem: Sending notifications to all volunteers assigned to an event required 
joining three tables (assignments → volunteers → users)

Solution: Used SQL JOINs with DISTINCT to handle the many-to-many relationship:

    SELECT DISTINCT u.id AS userId
    FROM assignments a
    JOIN volunteers v ON v.id = a.volunteer_id
    JOIN users u ON u.id = v.user_id
    WHERE a.event_id = ?


Challenge 2: Real-time vs Polling Trade-offs
----------------------------------------------
- Initially considered WebSockets for real-time notifications
- DECISION: Chose polling-based approach (REST GET requests) for simplicity
- TRADE-OFF: Slightly delayed notifications, but simpler architecture and 
  easier to debug
- FUTURE ENHANCEMENT: Could implement Server-Sent Events (SSE) or WebSockets 
  for true real-time


Challenge 3: Notification State Management
-------------------------------------------
- Frontend displays unread count and filters (all/unread/by-type)
- ISSUE: Backend doesn't currently support marking notifications as read 
  (local state only)
- LEARNING: Recognized this as technical debt - would implement PATCH endpoint 
  for production


Challenge 4: Targeting Specific Users
--------------------------------------
- Support for sending notifications either by volunteer name OR by event ID
- Required flexible querying logic to find the correct user_id via the 
  volunteer relationship


================================================================================
2. VOLUNTEER MATCHING SYSTEM - TECHNICAL CHALLENGES
================================================================================

ARCHITECTURE:
- Skill-based matching algorithm using set intersection
- Complex SQL queries with CTEs (Common Table Expressions) for finding 
  qualified volunteers
- Eligibility rules enforced both backend and frontend

KEY TECHNICAL CHALLENGES FACED:

Challenge 1: Complex Skill Matching with Proficiency Levels
------------------------------------------------------------
Had to handle three proficiency levels: basic, intermediate, advanced
Events specify required_level, volunteers have their proficiency

    CASE 
      WHEN ers.required_level = 'advanced' 
        THEN vs.proficiency = 'advanced'
      WHEN ers.required_level = 'intermediate' 
        THEN vs.proficiency IN ('intermediate', 'advanced')
      ELSE vs.proficiency IN ('basic', 'intermediate', 'advanced')
    END

PROBLEM: Needed to match volunteers who meet ALL skill requirements at the 
right proficiency level

SOLUTION: Used CTEs to:
1. Extract event's required skills
2. Find volunteers with matching skills and acceptable proficiency
3. Count matched skills and filter for complete matches


Challenge 2: Capacity Management
---------------------------------
- Events have limited capacity
- Need to prevent over-assignment while showing remaining spots

SOLUTION:
    COUNT(DISTINCT a.volunteer_id) as current_assignments
    -- Then calculate: capacity - current_assignments


Challenge 3: Preventing Duplicate Assignments
----------------------------------------------
- Same volunteer shouldn't be assigned to the same event twice

SOLUTION: Pre-check with SELECT before INSERT:

    const [existing] = await pool.query(
      "SELECT id FROM assignments WHERE volunteer_id = ? AND event_id = ?",
      [volunteerId, eventId]
    );
    if (existing.length > 0) {
      return res.status(400).json({ error: "Already assigned" });
    }


Challenge 4: Frontend Algorithm Efficiency
-------------------------------------------
    function skillOverlap(volunteer, event) {
      const v = new Set(volunteer?.skills || []);
      const req = event?.requiredSkills || [];
      return req.reduce((acc, s) => acc + (v.has(s) ? 1 : 0), 0);
    }

- Used JavaScript Sets for O(1) lookup instead of nested loops
- Memoized calculations with React's useMemo to avoid recalculating on every 
  render


================================================================================
3. USER AUTHENTICATION
================================================================================

IMPLEMENTATION:
- JWT (JSON Web Tokens) for stateless authentication
- Token stored in localStorage on frontend
- Authorization: Bearer <token> header for protected routes

CURRENT SETUP:

Token generation on login:
    const token = jwt.sign(
      { username: user.username },
      process.env.JWT_SECRET,
      { expiresIn: "1h" }
    );

Middleware (currently disabled for development):
    jwt.verify(token, process.env.JWT_SECRET, (err, user) => {
      if (err) return res.status(403).json({ message: "Invalid token" });
      req.user = user;
      next();
    });

SECURITY CONSIDERATIONS IMPLEMENTED:
1. Environment variables for JWT secret (never hardcoded)
2. Token expiration (1 hour) to limit exposure window
3. HTTPS enforcement via SSL certificates for production (see db.js SSL config)
4. Password security: In production, would use bcrypt for hashing (currently 
   plaintext for demo)

WHAT I WOULD IMPROVE:
- Implement refresh tokens for better UX (auto-renew without re-login)
- Add rate limiting on login endpoint to prevent brute force
- Use bcrypt for password hashing
- Implement role-based access control (RBAC) - currently all authenticated 
  users have same permissions


================================================================================
SYSTEM DESIGN QUESTIONS
================================================================================

REST API vs GraphQL - WHY I CHOSE REST
================================================================================

MY IMPLEMENTATION: REST API with Express.js

REASONING:
1. Simpler data model - Each endpoint has a clear, single purpose (get 
   volunteers, get events, create match)
2. No over-fetching concerns - My queries are already optimized to fetch 
   exactly what's needed
3. Easier caching - REST's predictable URLs work well with HTTP caching 
   (GET requests)
4. Team familiarity - Standard REST conventions are widely understood

WHEN GRAPHQL WOULD BE BETTER:
- If frontend needed flexible queries (e.g., "get event with volunteers, 
  their skills, and notification history")
- If mobile app needed different data shapes than web app
- If we had hundreds of related entities with complex relationships

REST STRENGTHS IN MY PROJECT:
- /api/volunteers - Simple resource fetching
- /api/events/:eventId/matches - Clear hierarchical relationship
- /api/match - Single action, predictable payload


================================================================================
SCALING FOR MANY USERS
================================================================================

CURRENT BOTTLENECKS:

Database Connection Pool:
    connectionLimit: 10  // Only 10 concurrent connections

SCALING STRATEGY:
- Increase pool size based on load testing
- Monitor with connection metrics
- Implement read replicas for SELECT queries (volunteers, events, notifications)

N+1 QUERY PROBLEM:
Avoided this by using JOINs with GROUP_CONCAT:

    SELECT v.*, GROUP_CONCAT(s.name) AS skills
    FROM volunteers v
    LEFT JOIN volunteer_skills vs ON vs.volunteer_id = v.id
    LEFT JOIN skills s ON s.id = vs.skill_id
    GROUP BY v.id

WITHOUT OPTIMIZATION: Would query skills for each volunteer separately 
(N+1 queries)

HORIZONTAL SCALING APPROACHES:

1. Stateless API Design
- JWT means no server-side session storage
- Any server instance can handle any request
- Can deploy behind load balancer (e.g., AWS ALB, NGINX)

2. Database Scaling:

Read Replicas:
    Primary (writes) → match creation, notification creation
    Replicas (reads) → fetch volunteers, fetch events, fetch notifications

Vertical Partitioning:
- Separate database for notifications (high write volume)
- Main database for volunteers/events (mostly reads)

3. Application Tier:
- Run multiple Express instances behind load balancer
- Use PM2 or Kubernetes for process management
- Health checks on /api/hello endpoint


================================================================================
SHARDING STRATEGY
================================================================================

SCENARIO: 10 million volunteers, 1 million events

SHARDING KEY SELECTION:

Option 1: Shard by Geographic Region
--------------------------------------
    Shard 1: volunteers/events in US-East
    Shard 2: volunteers/events in US-West
    Shard 3: volunteers/events in Europe

PROS:
- Natural data locality
- Events typically matched with local volunteers
- Reduced latency

CONS:
- Data imbalance (some regions have more volunteers)
- Cross-region matching requires cross-shard queries

Option 2: Shard by Volunteer ID
---------------------------------
    Shard 1: volunteer_id % 3 == 0
    Shard 2: volunteer_id % 3 == 1
    Shard 3: volunteer_id % 3 == 2

PROS: Even data distribution

CONS: Event matching requires scatter-gather across all shards

MY RECOMMENDATION: HYBRID APPROACH
1. Shard by geographic region (most queries are local)
2. Use global routing layer to handle cross-region queries
3. Cache frequently accessed data (popular events, top volunteers)


================================================================================
CACHING STRATEGY
================================================================================

WHAT I WOULD CACHE:

1. Volunteer Skills (Redis)
----------------------------
    // Key: volunteer:{id}:skills
    // TTL: 1 hour (skills don't change often)

    async function getVolunteerSkills(volunteerId) {
      const cached = await redis.get(`volunteer:${volunteerId}:skills`);
      if (cached) return JSON.parse(cached);
      
      const skills = await db.query(/* ... */);
      await redis.setex(`volunteer:${volunteerId}:skills`, 3600, 
                        JSON.stringify(skills));
      return skills;
    }

2. Event Details (Redis)
-------------------------
    // Key: event:{id}
    // TTL: 5 minutes (events updated less frequently)
    // INVALIDATE: On event update/deletion

3. Matching Results (Redis)
----------------------------
    // Key: matches:event:{eventId}
    // TTL: 30 seconds (capacity changes frequently)
    // INVALIDATE: On new assignment

4. HTTP Caching Headers
------------------------
    app.get('/api/volunteers', (req, res) => {
      res.set('Cache-Control', 'public, max-age=300'); // 5 minutes
      res.json(volunteers);
    });

5. CDN for Frontend Assets
---------------------------
- React bundle, CSS, images served from CloudFront/Cloudflare
- Reduces server load, improves global latency

CACHE INVALIDATION STRATEGY:

    // On volunteer skill update:
    await redis.del(`volunteer:${volunteerId}:skills`);

    // On new event assignment:
    await redis.del(`matches:event:${eventId}`);

    // On event update:
    await redis.del(`event:${eventId}`);


================================================================================
PERFORMANCE OPTIMIZATIONS I IMPLEMENTED
================================================================================

1. Database Indexing:
----------------------
Would add these indexes:

    CREATE INDEX idx_volunteer_skills 
      ON volunteer_skills(volunteer_id, skill_id);
    
    CREATE INDEX idx_event_skills 
      ON event_skills(event_id, skill_id);
    
    CREATE INDEX idx_assignments_event 
      ON assignments(event_id, status);
    
    CREATE INDEX idx_notifications_user 
      ON notifications(recipient_user_id, created_at);

2. Query Optimization:
----------------------
- Used GROUP_CONCAT to fetch related data in single query
- Used LEFT JOIN to avoid null-related bugs
- Used COALESCE for default values

3. Frontend Optimization:
-------------------------
- useMemo for expensive calculations (skill matching)
- useEffect with proper dependencies to avoid unnecessary re-renders
- Single Promise.all for parallel data fetching


================================================================================
ADDITIONAL TECHNICAL TALKING POINTS
================================================================================

ERROR HANDLING:
---------------
    try {
      const [rows] = await pool.query(/* ... */);
      res.json(rows);
    } catch (err) {
      console.error("Error fetching volunteers:", err);
      res.status(500).json({ error: "Failed to load volunteers" });
    }

- Consistent error responses
- Server-side logging for debugging
- User-friendly error messages on frontend

TESTING:
--------
- Jest for both frontend and backend
- Supertest for API integration tests
- Coverage reporting (see coverage/ directories)

ENVIRONMENT CONFIGURATION:
--------------------------
- .env for secrets (DB credentials, JWT secret, SSL certs)
- Different configs for dev/production
- SSL certificates for secure MySQL connections


================================================================================
SAMPLE INTERVIEW RESPONSES
================================================================================

Q: "Walk me through how your notification system works."
=========================================================

"The notification system uses a database-driven approach with a notifications 
table that references users and events. When an event assignment happens, the 
system queries the volunteers table to find the associated user_id, then 
inserts a notification record. The frontend polls this endpoint with the 
user's ID to fetch their notifications.

The main technical challenge was handling the many-to-many relationship 
between events and users through the volunteers and assignments tables. I used 
SQL JOINs with DISTINCT to efficiently query all users who needed to be 
notified.

For production, I'd enhance this with Server-Sent Events or WebSockets for 
real-time delivery, and implement proper read/unread state management on the 
backend instead of just in the frontend."


Q: "How would you scale this to handle 1 million concurrent users?"
====================================================================

"I'd approach this in layers:

APPLICATION TIER: The API is already stateless with JWT, so I can horizontally 
scale by running multiple Express instances behind a load balancer like AWS 
ALB. I'd use health checks on my /api/hello endpoint.

DATABASE TIER: I'd implement read replicas since most operations are reads - 
fetching volunteers, events, and notifications. Writes like creating matches 
and notifications would go to the primary, while reads could be distributed 
across replicas.

CACHING: I'd add Redis to cache volunteer skills and event details with 
appropriate TTLs and invalidation strategies. Matching results could also be 
cached for 30 seconds since they're computationally expensive.

CDN: Frontend assets would be served from a CDN to reduce origin load.

For the database specifically, I'd consider sharding by geographic region 
since volunteer matching is typically local, which would give us data locality 
benefits."


Q: "Why did you choose REST over GraphQL?"
===========================================

"I chose REST because my data model is relatively straightforward with clear 
resource boundaries - volunteers, events, notifications, and matches. Each 
endpoint has a single, well-defined purpose, and I'm not experiencing 
over-fetching problems.

REST also gave me better caching strategies - GET requests have predictable 
URLs that work well with HTTP caching and CDNs. The matching algorithm 
required custom SQL with CTEs and JOINs that would've been complex to express 
in GraphQL.

However, I can see GraphQL being beneficial if we expanded to mobile apps 
needing different data shapes, or if the frontend needed to compose complex 
queries like 'get event with all assigned volunteers, their complete skill 
profiles, and notification history' in a single request."


================================================================================
KEY TAKEAWAYS TO EMPHASIZE
================================================================================

1. PRACTICAL PROBLEM-SOLVING
   Made engineering trade-offs (polling vs WebSockets) based on project 
   constraints

2. SQL EXPERTISE
   Complex queries with CTEs, JOINs, GROUP_CONCAT, and proper indexing 
   considerations

3. PERFORMANCE AWARENESS
   Used memoization, parallel fetching, and efficient algorithms

4. SECURITY MINDSET
   JWT, environment variables, SSL, input validation

5. PRODUCTION READINESS
   Identified technical debt and articulated improvement paths

6. SCALABILITY THINKING
   Can discuss horizontal scaling, sharding, caching, and read replicas


================================================================================
GOOD LUCK WITH YOUR BLOOMBERG INTERVIEW!
================================================================================

Focus on explaining your thought process and the trade-offs you considered, 
not just the final implementation. Show that you understand not only how to 
build systems, but how to make them production-ready and scalable.
